{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) #Make full screen width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = !pip install --ignore-installed --upgrade tensorflow==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd; import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDirectory = '/home/ec2-user/SageMaker/'\n",
    "bucket_name = 'hilcorp-l48operations-plunger-lift-main'\n",
    "prefix = 'DataByAPI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::446356438225:role/service-role/hilcorp-l48operations-plunger-SagemakerPlungerRole-AC88IBLWFE8K\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-11 17:28:46    8.4 MiB DataByAPI/0506705008.csv\n",
      "2021-01-11 17:28:46   14.1 MiB DataByAPI/0506705009.csv\n",
      "2021-01-11 17:28:46    8.3 MiB DataByAPI/0506705016.csv\n",
      "2021-01-11 17:28:46    7.1 MiB DataByAPI/0506705027.csv\n",
      "2021-01-11 17:28:46    8.6 MiB DataByAPI/0506705031.csv\n",
      "\n",
      "[Errno 32] Broken pipe\n",
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='UTF-8'>\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "CPU times: user 36.3 ms, sys: 24.7 ms, total: 61 ms\n",
      "Wall time: 3.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Sync S3 Bucket wtih instance file store (Will replace when figure out how to stream S3 to tensorflow)\n",
    "!aws s3 ls s3://hilcorp-l48operations-plunger-lift-main/DataByAPI --recursive --human-readable --summarize | head -5\n",
    "!aws s3 sync s3://hilcorp-l48operations-plunger-lift-main/DataByAPI/ /home/ec2-user/SageMaker/DataByAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a new path which may be more efficient for training\n",
    "def csv_to_tensor(file_path):\n",
    "    #First read the file into a string\n",
    "    sCsv = tf.io.read_file(file_path)#, [tf.constant('-1')]*1)#.numpy().decode('utf-8')\n",
    "    #Then split the string into rows\n",
    "    rowSplit = tf.strings.split(sCsv, sep=b'\\r\\n', maxsplit=-1, name='SplitLines')\n",
    "    rowSplit = tf.strings.split(sCsv, sep=b'\\n', maxsplit=-1, name='SplitLines')\n",
    "    #The split that into rows and columns\n",
    "    colSplit = tf.strings.split(rowSplit, sep=b',', maxsplit=-1, name='SplitLines')\n",
    "    #Remove the header. The last line will be empty b/c the previous ended with the new line character. Convert to tensor\n",
    "    colSplit = colSplit[1:-1,4:].to_tensor()#This removes the column header and the empty last row\n",
    "    #Replace empty strings with -1\n",
    "    colSplit = tf.where(tf.equal(colSplit, b''), b'-1', colSplit)\n",
    "    #Convert from string to float32\n",
    "    outTensor = tf.strings.to_number(colSplit, out_type = tf.dtypes.float32, name = 'f32TensorCsv')\n",
    "\n",
    "    return outTensor\n",
    "\n",
    "#This is a new path which may be more efficient for training\n",
    "def replaceNanOrInf(X):\n",
    "    bMask = tf.math.logical_or(tf.math.is_nan(X),tf.math.is_inf(X))\n",
    "    return tf.where(bMask,-1.,X)\n",
    "\n",
    "def process_path(file_path):\n",
    "\n",
    "    # tf.print('file path: {}'.format(file_path))\n",
    "\n",
    "    PLUNGER_SPEED_loc = 1\n",
    "\n",
    "    GAS_PER_CYCLE_loc = 23\n",
    "    FLOW_LENGTH_loc = 12\n",
    "    SHUTIN_LENGTH_loc = 11\n",
    "    LEAKING_VALVE_loc = 29\n",
    "\n",
    "    FLOW_RATE_END_FLOW_loc = 2\n",
    "    CS_MINUS_LN_SI_loc = 75\n",
    "    PERCENT_CL_END_FLOW_loc = 76\n",
    "    inputTensor = csv_to_tensor(file_path)\n",
    "\n",
    "    inputTensor = tf.clip_by_value(inputTensor, -1e6, 1e6, name='ClippedInput')\n",
    "\n",
    "    #Remove non-physical rows\n",
    "    bMask = tf.greater(inputTensor[:,SHUTIN_LENGTH_loc],1)\n",
    "    bMask = tf.logical_and(bMask,tf.greater(inputTensor[:,FLOW_LENGTH_loc],1))\n",
    "    bMask = tf.logical_and(bMask,tf.greater(inputTensor[:,PLUNGER_SPEED_loc],1))\n",
    "    bMask = tf.logical_and(bMask,tf.less(inputTensor[:,LEAKING_VALVE_loc],0.5))\n",
    "    bMask = tf.logical_and(bMask,tf.greater(inputTensor[:,CS_MINUS_LN_SI_loc],1))\n",
    "\n",
    "    inputTensor = tf.boolean_mask(inputTensor,bMask)\n",
    "\n",
    "    Xpolicy = tf.stack((\n",
    "                 inputTensor[1:-1,PERCENT_CL_END_FLOW_loc],\n",
    "                 inputTensor[2:,CS_MINUS_LN_SI_loc]\n",
    "                 ),\n",
    "                 axis = 1)\n",
    "\n",
    "    X = tf.concat((inputTensor[0:-2,:], #Results\n",
    "                 Xpolicy), axis = 1) #Next Cycle's controller value\n",
    "\n",
    "    #This calcualtes the MCFD for the cycle definition which ends with a plunger arrival.\n",
    "    correctedMCFD = inputTensor[1:-1,GAS_PER_CYCLE_loc]/((inputTensor[1:-1,FLOW_LENGTH_loc]+inputTensor[2:,SHUTIN_LENGTH_loc])/86400.)\n",
    "\n",
    "    Y = tf.stack((correctedMCFD,\n",
    "                 inputTensor[2:,1]), #Plunger speed\n",
    "                 axis = 1,\n",
    "                #  name = r'Y_'+str(file_path)\n",
    "                 )\n",
    "\n",
    "\n",
    "    X = replaceNanOrInf(X)\n",
    "    Y = replaceNanOrInf(Y)\n",
    "\n",
    "    X = tf.clip_by_value(X,-1e2,1e6)\n",
    "    Y = tf.clip_by_value(Y,0.,2000.)\n",
    "\n",
    "    tf.debugging.check_numerics(X, 'X error, file: {} '.format(file_path))\n",
    "    tf.debugging.check_numerics(Y, 'Y error, file: {} '.format(file_path))\n",
    "\n",
    "    return X, Y, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/radix-ai-blog/tensorflow-sagemaker-d17774417f08\n",
    "#https://stackoverflow.com/questions/62513518/how-to-save-a-tensor-to-tfrecord\n",
    "\"\"\" \n",
    "Converts data to TFRecords file format \n",
    "\"\"\"\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def convert_ds_to_TFRecord(ds, num_elements, name, directory):\n",
    "#     num_elements = ds.reduce(np.int64(0), lambda x, _: x + 1).numpy()\n",
    "    filename = os.path.join(directory, f'{name}-{num_elements}-Records.tfrecords')\n",
    "    print(f'Writing {filename}')\n",
    "    with tf.io.TFRecordWriter(filename) as writer: \n",
    "        for X, Y, path in tqdm.tqdm_notebook(ds):\n",
    "            UWI = path.numpy()[-14:-4]\n",
    "            num_time_steps = X.shape[0]\n",
    "            # Serialize the tensors\n",
    "            X_raw = X.numpy().tostring()\n",
    "            Y_raw = Y.numpy().tostring()\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'UWI': _bytes_feature(UWI),\n",
    "                    'X_raw': _bytes_feature(X_raw),\n",
    "                    'Y_raw': _bytes_feature(Y_raw),\n",
    "                    'num_time_steps': _int64_feature(num_time_steps)\n",
    "                    }))\n",
    "\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5138 wells to train on\n"
     ]
    }
   ],
   "source": [
    "lFileNames = !ls /home/ec2-user/SageMaker/DataByAPI\n",
    "num_examples = len(lFileNames)\n",
    "print(f'{num_examples} wells to train on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFileNames = [rootDirectory + r'DataByAPI/*.csv']\n",
    "buffer_size = 64\n",
    "batch_size = 2\n",
    "\n",
    "raw_dataset = tf.data.Dataset.list_files(DataFileNames)\n",
    "allWellDs = raw_dataset.map(process_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/ec2-user/SageMaker/TFRecordFiles/DatasetOneExamplePerWellWithUWI-5138-Records.tfrecords\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1ee27e400c4535ad73ad40f23a8035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5135.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# convert_ds_to_TFRecord(allWellDs,num_examples,'DatasetOneExamplePerWellWithUWI',r's3://hilcorp-l48operations-plunger-lift-main/TFRecordFiles/')\n",
    "convert_ds_to_TFRecord(allWellDs,num_examples,'DatasetOneExamplePerWellWithUWI',r'/home/ec2-user/SageMaker/TFRecordFiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 13.4 GiB/26.1 GiB (60.3 MiB/s) with 1 file(s) remaining  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: TFRecordFiles/DatasetOneExamplePerWellWithUWI-5138-Records.tfrecords to s3://hilcorp-l48operations-plunger-lift-main/TFRecordFiles/DatasetOneExamplePerWellWithUWI-5138-Records.tfrecords\n"
     ]
    }
   ],
   "source": [
    "# !aws s3 cp /home/ec2-user/SageMaker/TFRecordFiles/DatasetOneExamplePerWellWithUWI-5138-Records.tfrecords s3://hilcorp-l48operations-plunger-lift-main/TFRecordFiles/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fname = 's3://hilcorp-l48operations-plunger-lift-main/DataByAPI/3003921089.csv'\n",
    "df = pd.read_csv(fname, nrows = 100).head(1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.lib.io import file_io\n",
    "print(file_io.stat(fname))\n",
    "# print(file_io.stat('s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data')) #This is a public S3 file\n",
    "# file_io.stat('s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
