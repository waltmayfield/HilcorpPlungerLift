{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Found existing installation: protobuf 3.11.4\n",
      "Uninstalling protobuf-3.11.4:\n",
      "  Would remove:\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/lib/python2.7/site-packages/protobuf-3.11.4-py2.7.egg-info\n",
      "Proceed (y/n)?   Successfully uninstalled protobuf-3.11.4\n",
      "yes: standard output: Broken pipe\n",
      "yes: write error\n",
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
      "yes: standard output: Broken pipe\n",
      "yes: write error\n",
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Found existing installation: tensorflow-gpu 2.1.0\n",
      "Uninstalling tensorflow-gpu-2.1.0:\n",
      "  Would remove:\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/bin/estimator_ckpt_converter\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/bin/saved_model_cli\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/bin/tensorboard\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/bin/tf_upgrade_v2\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/bin/tflite_convert\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/bin/toco\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/bin/toco_from_protos\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/lib/python2.7/site-packages/tensorflow/*\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/lib/python2.7/site-packages/tensorflow_core/*\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/lib/python2.7/site-packages/tensorflow_gpu-2.1.0.dist-info/*\n",
      "Proceed (y/n)?   Successfully uninstalled tensorflow-gpu-2.1.0\n",
      "yes: standard output: Broken pipe\n",
      "yes: write error\n",
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Collecting tensorflow-gpu\n",
      "  Using cached tensorflow_gpu-2.1.0-cp27-cp27mu-manylinux2010_x86_64.whl (421.8 MB)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.14.0-cp27-cp27mu-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.11.0.tar.gz (110 kB)\n",
      "\u001b[K     |████████████████████████████████| 110 kB 29.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.34.0-cp27-cp27mu-manylinux2010_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 26.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.12.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting backports.weakref>=1.0rc1; python_version < \"3.4\"\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Using cached tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Collecting enum34>=1.1.6; python_version < \"3.4\"\n",
      "  Downloading enum34-1.1.10-py2-none-any.whl (11 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8.tar.gz (289 kB)\n",
      "\u001b[K     |████████████████████████████████| 289 kB 62.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<2.0,>=1.16.0\n",
      "  Downloading numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl (17.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.0 MB 60.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-2.3.2.tar.gz (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.6\n",
      "  Downloading google_pasta-0.2.0-py2-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 8.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting wheel; python_version < \"3\"\n",
      "  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting mock>=2.0.0; python_version < \"3\"\n",
      "  Downloading mock-3.0.5-py2.py3-none-any.whl (25 kB)\n",
      "Collecting functools32>=3.2.3; python_version < \"3\"\n",
      "  Downloading functools32-3.2.3-2.tar.gz (31 kB)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Using cached tensorboard-2.1.0-py2-none-any.whl (3.8 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting scipy==1.2.2; python_version < \"3\"\n",
      "  Using cached scipy-1.2.2-cp27-cp27mu-manylinux1_x86_64.whl (24.8 MB)\n",
      "Collecting futures>=2.2.0; python_version < \"3.2\"\n",
      "  Downloading futures-3.3.0-py2-none-any.whl (16 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-2.10.0-cp27-cp27mu-manylinux1_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 49.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting funcsigs>=1; python_version < \"3.3\"\n",
      "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Downloading setuptools-44.1.1-py2.py3-none-any.whl (583 kB)\n",
      "\u001b[K     |████████████████████████████████| 583 kB 36.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.1.1-py2.py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting rsa<4.6; python_version < \"3.6\"\n",
      "  Using cached rsa-4.5-py2.py3-none-any.whl (36 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Building wheels for collected packages: absl-py, wrapt, keras-applications, opt-einsum, functools32, gast, termcolor\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.11.0-py2-none-any.whl size=127442 sha256=dddc678a410e5965442d7341349bd96d1f42ff22b72bc6a6177fe1b29a168ca8\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/be/12/8b/0c41b135a6383624dedb48fa1c3a4c99f7961edafcb42e8189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp27-cp27mu-linux_x86_64.whl size=63754 sha256=523ae599c41bbd34f12681d912ae00782a52cce945a791874061479dc0a80842\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/5b/d8/8e/81a83cb5321b940a954996f5b57fddc8976e712b3ac3a1a54b\n",
      "  Building wheel for keras-applications (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-applications: filename=Keras_Applications-1.0.8-py2-none-any.whl size=50701 sha256=7bbde717f7383ca1d3d2d98c167ddb46fc06228e37fd4bebec7af16e0a1ecd9e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/71/a0/64/e2e0c93740e0460f4b7f036141b7c73b5e44ff38f690ddff9f\n",
      "  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opt-einsum: filename=opt_einsum-2.3.2-py2-none-any.whl size=49881 sha256=85ad7434b317d11e6edfc067a675ca865179431cfe1e5943e54dc428824d4fe2\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ef/c4/c2/d0b07dd2a54f4d583a5de0e6ce5eb7a1832961b9a10d1ec953\n",
      "  Building wheel for functools32 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for functools32: filename=functools32-3.2.3.post2-py2-none-any.whl size=14556 sha256=25223c4d3c35d898db3be65bf2cf2957f11fe15ed73c90b7852e61ef70aea971\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c2/ea/a3/25af52265fad6418a74df0b8d9ca8b89e0b3735dbd4d0d3794\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py2-none-any.whl size=7539 sha256=6205289104a3bcd43815357b5cc96aa4dde1ca85700b8d1f86a41ed62ad80c64\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/0f/10/f7/29260ef8a721b90061c8c70a4f0130a64036e8dafe15acc097\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py2-none-any.whl size=4831 sha256=0700c3d60cd227f4e7e06e2196e2a124615e609ec7fb5c60b8feba8fe45c13e1\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/48/54/87/2f4d1a48c87e43906477a3c93d9663c49ca092046d5a4b00b4\n",
      "Successfully built absl-py wrapt keras-applications opt-einsum functools32 gast termcolor\n",
      "Installing collected packages: six, protobuf, enum34, absl-py, futures, grpcio, backports.weakref, tensorflow-estimator, astor, wrapt, numpy, h5py, keras-applications, opt-einsum, google-pasta, wheel, keras-preprocessing, funcsigs, mock, functools32, gast, setuptools, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, urllib3, idna, chardet, certifi, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, werkzeug, tensorboard, termcolor, scipy, tensorflow-gpu\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.12.0\n",
      "    Uninstalling six-1.12.0:\n",
      "      Successfully uninstalled six-1.12.0\n",
      "  Attempting uninstall: enum34\n",
      "    Found existing installation: enum34 1.1.6\n",
      "    Uninstalling enum34-1.1.6:\n",
      "      Successfully uninstalled enum34-1.1.6\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.11.0\n",
      "    Uninstalling absl-py-0.11.0:\n",
      "      Successfully uninstalled absl-py-0.11.0\n",
      "  Attempting uninstall: futures\n",
      "    Found existing installation: futures 3.3.0\n",
      "    Uninstalling futures-3.3.0:\n",
      "      Successfully uninstalled futures-3.3.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.33.2\n",
      "    Uninstalling grpcio-1.33.2:\n",
      "      Successfully uninstalled grpcio-1.33.2\n",
      "  Attempting uninstall: backports.weakref\n",
      "    Found existing installation: backports.weakref 1.0.post1\n",
      "    Uninstalling backports.weakref-1.0.post1:\n",
      "      Successfully uninstalled backports.weakref-1.0.post1\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
      "  Attempting uninstall: astor\n",
      "    Found existing installation: astor 0.8.1\n",
      "    Uninstalling astor-0.8.1:\n",
      "      Successfully uninstalled astor-0.8.1\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.5\n",
      "    Uninstalling numpy-1.16.5:\n",
      "      Successfully uninstalled numpy-1.16.5\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "  Attempting uninstall: keras-applications\n",
      "    Found existing installation: Keras-Applications 1.0.8\n",
      "    Uninstalling Keras-Applications-1.0.8:\n",
      "      Successfully uninstalled Keras-Applications-1.0.8\n",
      "  Attempting uninstall: opt-einsum\n",
      "    Found existing installation: opt-einsum 2.3.2\n",
      "    Uninstalling opt-einsum-2.3.2:\n",
      "      Successfully uninstalled opt-einsum-2.3.2\n",
      "  Attempting uninstall: google-pasta\n",
      "    Found existing installation: google-pasta 0.2.0\n",
      "    Uninstalling google-pasta-0.2.0:\n",
      "      Successfully uninstalled google-pasta-0.2.0\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.33.6\n",
      "    Uninstalling wheel-0.33.6:\n",
      "      Successfully uninstalled wheel-0.33.6\n",
      "  Attempting uninstall: keras-preprocessing\n",
      "    Found existing installation: Keras-Preprocessing 1.1.0\n",
      "    Uninstalling Keras-Preprocessing-1.1.0:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
      "  Attempting uninstall: funcsigs\n",
      "    Found existing installation: funcsigs 1.0.2\n",
      "    Uninstalling funcsigs-1.0.2:\n",
      "      Successfully uninstalled funcsigs-1.0.2\n",
      "  Attempting uninstall: mock\n",
      "    Found existing installation: mock 3.0.5\n",
      "    Uninstalling mock-3.0.5:\n",
      "      Successfully uninstalled mock-3.0.5\n",
      "  Attempting uninstall: functools32\n",
      "    Found existing installation: functools32 3.2.3.post2\n",
      "\u001b[31mERROR: Cannot uninstall 'functools32'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p27/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "yes: standard output: Broken pipe\n",
      "yes: write error\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip         # pip 19.0 or higher is required for TF 2\n",
    "# !pip install --upgrade setuptools  # Otherwise you'll get annoying warnings about bad installs\n",
    "!yes | pip uninstall protobuf\n",
    "!yes | pip uninstall tensorflow\n",
    "!yes | pip uninstall tensorflow-gpu\n",
    "!yes | pip install --upgrade --force-reinstall tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p27/gpu_cuda10.0/lib/python2.7/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name=u'/physical_device:GPU:0', device_type=u'GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) #Make full screen width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.4.0\n",
      "  Using cached tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7 MB)\n",
      "Collecting absl-py~=0.10\n",
      "  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Using cached grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Using cached h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.14.0-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting six~=1.15.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tensorboard~=2.4\n",
      "  Using cached tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.0-py3-none-any.whl (12 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "Collecting importlib-metadata\n",
      "  Using cached importlib_metadata-3.4.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.7-py3-none-any.whl (34 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Using cached setuptools-51.1.2-py3-none-any.whl (784 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
      "  Using cached tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting wheel~=0.35\n",
      "  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.4.0-py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: urllib3, pyasn1, idna, chardet, certifi, zipp, typing-extensions, six, setuptools, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 4.0.1 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n",
      "spyder 4.0.1 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "tensorflow-serving-api 2.1.0 requires tensorflow~=2.1.0, but you have tensorflow 2.4.0 which is incompatible.\n",
      "awscli 1.18.197 requires rsa<=4.5.0,>=3.1.2; python_version != \"3.4\", but you have rsa 4.7 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.0 certifi-2020.12.5 chardet-4.0.0 flatbuffers-1.12 gast-0.3.3 google-auth-1.24.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 idna-2.10 importlib-metadata-3.4.0 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.7 setuptools-51.1.2 six-1.15.0 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.2 werkzeug-1.0.1 wheel-0.36.2 wrapt-1.12.1 zipp-3.4.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = !pip install --ignore-installed --upgrade tensorflow==2.4.0\n",
    "_ = !pip install wandb\n",
    "# _ = !pip install dirsync\n",
    "import wandb\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re; import sys; import importlib; import tqdm; import os; import time; import glob\n",
    "import numpy as np; import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Metrics' from '/home/ec2-user/SageMaker/PythonFiles/Metrics.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is to import local modules\n",
    "sys.path.append('/home/ec2-user/SageMaker/PythonFiles')\n",
    "import FunctionsTF as F\n",
    "importlib.reload(F) #Incase need to reload module after change\n",
    "import Metrics as M\n",
    "importlib.reload(M) #Incase need to reload module after change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeDirectory = '/home/ec2-user/SageMaker/'\n",
    "\n",
    "model_name = r'20201216_460k_Param_LSTM_Skip_resBlock_311Epoch.h5'\n",
    "model_save_location = homeDirectory + r'Models/' + model_name\n",
    "output_model_save_location = homeDirectory + r'Models/' + r'20201216_460k_Param_LSTM_Skip_resBlock.h5'\n",
    "\n",
    "historyPath = homeDirectory + r'LossCurves/' + r'20201216History.csv'\n",
    "\n",
    "bucket_name = 'hilcorp-l48operations-plunger-lift-main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a new hitory df if one does not already exist\n",
    "if not os.path.isfile(historyPath):\n",
    "    print('Creating new history DF at {}'.format(time.localtime()))\n",
    "    pd.DataFrame(columns = ['loss', 'MCF_metric', 'plunger_speed_metric', 'val_loss', 'val_MCF_metric', 'val_plunger_speed_metric']).to_csv(historyPath, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, None, 79)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, None, 79)     316         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, None, 32)     14336       batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, None, 32)     14336       batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, None, 111)    0           lstm_6[0][0]                     \n",
      "                                                                 batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, 111)    0           lstm_7[0][0]                     \n",
      "                                                                 batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_116 (Dense)               (None, None, 128)    14336       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_130 (Dense)               (None, None, 128)    14336       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_117 (Dense)               (None, None, 128)    16512       dense_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_131 (Dense)               (None, None, 128)    16512       dense_130[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, None, 128)    512         dense_117[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, None, 128)    512         dense_131[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, None, 128)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, None, 128)    0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_118 (Dense)               (None, None, 128)    16512       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_132 (Dense)               (None, None, 128)    16512       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, None, 128)    512         dense_118[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, None, 128)    512         dense_132[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, None, 128)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, None, 128)    0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_52 (TensorFlowO (None, None, 128)    0           activation_105[0][0]             \n",
      "                                                                 dense_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_58 (TensorFlowO (None, None, 128)    0           activation_117[0][0]             \n",
      "                                                                 dense_130[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_119 (Dense)               (None, None, 128)    16512       tf_op_layer_Add_52[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_133 (Dense)               (None, None, 128)    16512       tf_op_layer_Add_58[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, None, 128)    512         dense_119[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, None, 128)    512         dense_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, None, 128)    0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, None, 128)    0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_120 (Dense)               (None, None, 128)    16512       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_134 (Dense)               (None, None, 128)    16512       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, None, 128)    512         dense_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, None, 128)    512         dense_134[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, None, 128)    0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, None, 128)    0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_53 (TensorFlowO (None, None, 128)    0           activation_107[0][0]             \n",
      "                                                                 tf_op_layer_Add_52[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_59 (TensorFlowO (None, None, 128)    0           activation_119[0][0]             \n",
      "                                                                 tf_op_layer_Add_58[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_121 (Dense)               (None, None, 128)    16512       tf_op_layer_Add_53[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_135 (Dense)               (None, None, 128)    16512       tf_op_layer_Add_59[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, None, 128)    512         dense_121[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, None, 128)    512         dense_135[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, None, 128)    0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, None, 128)    0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_122 (Dense)               (None, None, 128)    16512       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_136 (Dense)               (None, None, 128)    16512       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, None, 128)    512         dense_122[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, None, 128)    512         dense_136[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, None, 128)    0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, None, 128)    0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_54 (TensorFlowO (None, None, 128)    0           activation_109[0][0]             \n",
      "                                                                 tf_op_layer_Add_53[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_60 (TensorFlowO (None, None, 128)    0           activation_121[0][0]             \n",
      "                                                                 tf_op_layer_Add_59[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_123 (Dense)               (None, None, 128)    16512       tf_op_layer_Add_54[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_137 (Dense)               (None, None, 128)    16512       tf_op_layer_Add_60[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, None, 128)    512         dense_123[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, None, 128)    512         dense_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, None, 128)    0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, None, 128)    0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_124 (Dense)               (None, None, 128)    16512       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_138 (Dense)               (None, None, 128)    16512       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, None, 128)    512         dense_124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, None, 128)    512         dense_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, None, 128)    0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, None, 128)    0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_55 (TensorFlowO (None, None, 128)    0           activation_111[0][0]             \n",
      "                                                                 tf_op_layer_Add_54[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_61 (TensorFlowO (None, None, 128)    0           activation_123[0][0]             \n",
      "                                                                 tf_op_layer_Add_60[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_125 (Dense)               (None, None, 128)    16512       tf_op_layer_Add_55[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_139 (Dense)               (None, None, 128)    16512       tf_op_layer_Add_61[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, None, 128)    512         dense_125[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, None, 128)    512         dense_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, None, 128)    0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, None, 128)    0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_126 (Dense)               (None, None, 128)    16512       activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_140 (Dense)               (None, None, 128)    16512       activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, None, 128)    512         dense_126[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, None, 128)    512         dense_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, None, 128)    0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, None, 128)    0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_56 (TensorFlowO (None, None, 128)    0           activation_113[0][0]             \n",
      "                                                                 tf_op_layer_Add_55[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_62 (TensorFlowO (None, None, 128)    0           activation_125[0][0]             \n",
      "                                                                 tf_op_layer_Add_61[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_127 (Dense)               (None, None, 128)    16512       tf_op_layer_Add_56[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_141 (Dense)               (None, None, 128)    16512       tf_op_layer_Add_62[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, None, 128)    512         dense_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, None, 128)    512         dense_141[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, None, 128)    0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, None, 128)    0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_128 (Dense)               (None, None, 128)    16512       activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_142 (Dense)               (None, None, 128)    16512       activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, None, 128)    512         dense_128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, None, 128)    512         dense_142[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, None, 128)    0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, None, 128)    0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_57 (TensorFlowO (None, None, 128)    0           activation_115[0][0]             \n",
      "                                                                 tf_op_layer_Add_56[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_63 (TensorFlowO (None, None, 128)    0           activation_127[0][0]             \n",
      "                                                                 tf_op_layer_Add_62[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_129 (Dense)               (None, None, 1)      129         tf_op_layer_Add_57[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_143 (Dense)               (None, None, 1)      129         tf_op_layer_Add_63[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, None, 2)      0           dense_129[0][0]                  \n",
      "                                                                 dense_143[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 466,494\n",
      "Trainable params: 460,192\n",
      "Non-trainable params: 6,302\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_save_location, compile = False, custom_objects = {'LeakyReLU' : LeakyReLU()})\n",
    "model.summary()# tf.keras.utils.plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ec2-user/SageMaker/TFRecordFiles/DatasetOneExamplePerWellWithUWI-5138-Records.tfrecords']\n"
     ]
    }
   ],
   "source": [
    "list_of_files = glob.glob('/home/ec2-user/SageMaker/TFRecordFiles/*') # * means all if need specific format then *.csv\n",
    "latest_file = max(list_of_files, key=os.path.getctime) #This gets the most recently uploaded TF Record File\n",
    "lTFRecordFiles = [latest_file]\n",
    "print(lTFRecordFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.1\n",
    "batch_size = 2\n",
    "num_parallel_calls = 8\n",
    "buffer_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_data_items(filenames):\n",
    "    'Counts the records in each file name'\n",
    "    n = [int(re.compile(r\"-([0-9]*)-Records\\.\").search(filename).group(1)) for filename in filenames]\n",
    "#     print(n)\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = count_data_items(lTFRecordFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training wells: 4624, Validation wells: 514 of total wells 5138\n"
     ]
    }
   ],
   "source": [
    "numTrainWells = int(np.floor(num_examples*(1.-validation_split)))\n",
    "numValidWells = num_examples - numTrainWells\n",
    "print(f\"Number of training wells: {numTrainWells}, Validation wells: {numValidWells} of total wells {num_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(lTFRecordFiles)\n",
    "# allWellDs = allWellDs.map(lambda x, y: (x[:100,:],y[:100,:]))#This is just for testing purposes to trim X for shorter computation\n",
    "\n",
    "trainDs = raw_dataset.skip(numValidWells)\n",
    "trainDs = trainDs.map(F.parse_raw_examples_UWI, num_parallel_calls=num_parallel_calls)\n",
    "trainDs = trainDs.map(lambda x, y, UWIs: (x,y))#This is to remove the UWI which is not useful in trianing\n",
    "\n",
    "trainDs = trainDs.map(lambda x, y: (tf.reverse(x, axis = [0]),tf.reverse(y, axis = [0])))#This is to have leading instead of trailing 0s. Reverse the time direction\n",
    "trainDs = trainDs.padded_batch(batch_size, padded_shapes=([None,79],[None,2]))# Add the 0s behind the example\n",
    "trainDs = trainDs.map(lambda x, y: (tf.reverse(x, axis = [1]),tf.reverse(y, axis = [1]))) #Reverse the time to the correct direction\n",
    "trainDs = trainDs.prefetch(buffer_size)\n",
    "# trainDs = trainDs.cache(r'./')\n",
    "\n",
    "validDs = raw_dataset.take(numValidWells)\n",
    "validDs = validDs.map(F.parse_raw_examples_UWI, num_parallel_calls=num_parallel_calls)\n",
    "validDs = validDs.map(lambda x, y, UWIs: (x,y))#This is to remove the UWI which is not useful in trianing\n",
    "validDs = validDs.map(lambda x, y: (tf.reverse(x, axis = [0]),tf.reverse(y, axis = [0])))\n",
    "validDs = validDs.padded_batch(batch_size, padded_shapes=([None,79],[None,2]))\n",
    "validDs = validDs.map(lambda x, y: (tf.reverse(x, axis = [1]),tf.reverse(y, axis = [1])))\n",
    "validDs = validDs.prefetch(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a6cb92ecbd414fbedfdc251af1f72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm.tqdm_notebook(trainDs.take(5)): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187c669b967245d2b3e2d3c6366dda7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm.tqdm_notebook(validDs.take(5)): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 11 20:54:29 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   35C    P0    23W / 300W |      2MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = int(np.ceil(numTrainWells/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,historyPath):\n",
    "        self.historyPath = historyPath\n",
    "    def on_epoch_end(self,epoch,logs=None):#This saves the loss data\n",
    "        lossDf = pd.DataFrame(logs, index = [0]) #Turns logs into dataframe\n",
    "        lossDf.to_csv(self.historyPath, mode = 'a', header = False, index = False) #Appends to existing csv file\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(model_save_location, \n",
    "                                   monitor = 'loss', \n",
    "                                   save_best_only=False, \n",
    "                                   save_weights_only = False,\n",
    "                                   verbose=1)\n",
    "\n",
    "terminateOnNaN = keras.callbacks.TerminateOnNaN()\n",
    "log_results = EpochLogger(historyPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "   5/2312 [..............................] - ETA: 34:03:00 - loss: 19638.5912 - MCF_metric: 1599.3271 - plunger_speed_metric: 37593.5543"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-bb5c64811ab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mlog_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mterminateOnNaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             ]\n\u001b[1;32m     16\u001b[0m           )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "optimizer = Adam(lr = 1e-3)\n",
    "model.compile(loss=M.custom_loss, optimizer=optimizer, metrics = [M.MCF_metric, M.plunger_speed_metric])\n",
    "\n",
    "model.fit(x = trainDs.repeat(epochs),\n",
    "          validation_data=validDs,\n",
    "          epochs = epochs,\n",
    "          steps_per_epoch = steps_per_epoch,\n",
    "          use_multiprocessing=False,\n",
    "          callbacks = [\n",
    "            log_results,\n",
    "            model_checkpoint,\n",
    "            terminateOnNaN\n",
    "            ]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
